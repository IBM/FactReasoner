{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1dc791c",
   "metadata": {},
   "source": [
    "### Example of FactReasoner Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "parent_dir = str(Path(os.getcwd()).resolve().parent)\n",
    "sys.path.insert(0, parent_dir)\n",
    "print(parent_dir)\n",
    "\n",
    "from src.fact_reasoner.context_retriever import ContextRetriever\n",
    "from src.fact_reasoner.atom_extractor import AtomExtractor\n",
    "from src.fact_reasoner.atom_reviser import AtomReviser\n",
    "from src.fact_reasoner.query_builder import QueryBuilder\n",
    "from src.fact_reasoner.context_summarizer import ContextSummarizer\n",
    "from src.fact_reasoner.nli_extractor import NLIExtractor\n",
    "from src.fact_reasoner.factreasoner import FactReasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f60728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and backend\n",
    "model_id = \"llama-3.3-70b-instruct\"\n",
    "cache_dir = None # \"my_database.db\"\n",
    "backend = \"rits\"\n",
    "\n",
    "# Define the input file and merlin path (change to ...)\n",
    "input_file = \"/home/radu/git/FactReasoner/examples/flaherty_wikipedia.json\"\n",
    "merlin_path = \"/home/radu/git/fm-factual/lib/merlin\"\n",
    "\n",
    "# Create the pipeline modules\n",
    "query_builder = QueryBuilder(model_id=model_id, prompt_version=\"v1\", backend=backend)\n",
    "context_retriever = ContextRetriever(service_type=\"google\", top_k=5, cache_dir=cache_dir)\n",
    "context_summarizer = ContextSummarizer(model_id=model_id, prompt_version=\"v1\", backend=backend)\n",
    "atom_extractor = AtomExtractor(model_id=model_id, backend=backend)\n",
    "atom_reviser = AtomReviser(model_id=model_id, backend=backend)\n",
    "nli_extractor = NLIExtractor(model_id=model_id, prompt_version=\"v1\", backend=backend)\n",
    "\n",
    "# Create the FactReasoner pipeline\n",
    "pipeline = FactReasoner(\n",
    "    context_retriever=context_retriever,\n",
    "    context_summarizer=context_summarizer,\n",
    "    atom_extractor=atom_extractor,\n",
    "    atom_reviser=atom_reviser,\n",
    "    nli_extractor=nli_extractor,\n",
    "    query_builder=query_builder,\n",
    "    merlin_path=merlin_path,\n",
    ")\n",
    "\n",
    "# Load the problem instance from a file\n",
    "json_file = input_file\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Instantiate the pipeline from a data file\n",
    "pipeline.from_dict_with_contexts(data)\n",
    "\n",
    "# Build the FactReasoner pipeline (FR2 version)\n",
    "pipeline.build(\n",
    "    has_atoms=True,\n",
    "    has_contexts=True,\n",
    "    revise_atoms=False,\n",
    "    remove_duplicates=True,\n",
    "    contexts_per_atom_only=False,\n",
    "    rel_atom_context=True, \n",
    "    rel_context_context=False,\n",
    "    text_only=False\n",
    ")\n",
    "\n",
    "# Compute the marginals\n",
    "results, marginals = pipeline.score()\n",
    "print(f\"[FactReasoner] Marginals: {marginals}\")\n",
    "print(f\"[FactReasoner] Results: {results}\")\n",
    "print(f\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

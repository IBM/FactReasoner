{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c4f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from statistics import mean\n",
    "\n",
    "import dill\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.fact_reasoner.comprehensiveness import ComprehensivenessResult\n",
    "\n",
    "\n",
    "def load_results(filename: str) -> tuple[dict, list[list[ComprehensivenessResult]]]:\n",
    "    load_dotenv()\n",
    "    RESULTS_PATH = os.environ[\"RESULTS_PATH\"]\n",
    "\n",
    "    save_file_path = f\"{RESULTS_PATH}/{filename}\"\n",
    "    if not os.path.exists(save_file_path):\n",
    "        raise FileNotFoundError(f\"File {save_file_path} doesn't exist!\")\n",
    "    with open(save_file_path, \"rb\") as f:\n",
    "        experiment_data = dill.load(f)\n",
    "\n",
    "    return experiment_data[\"args\"], experiment_data[\"results\"]\n",
    "\n",
    "\n",
    "def compute_score_mean(\n",
    "    all_results: list[list[ComprehensivenessResult]], score_key: str | None = None\n",
    "):\n",
    "    scores = []\n",
    "    for results in all_results:\n",
    "        for result in results:\n",
    "            if score_key is None:\n",
    "                scores.append(result[\"comprehensiveness_eval_main_score\"])\n",
    "            else:\n",
    "                scores.append(result[\"comprehensiveness_eval_results\"][score_key])  # type: ignore\n",
    "    return mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c35f1f",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c57e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_VERSION = \"v12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0c8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "load_dotenv()\n",
    "RESULTS_PATH = os.environ[\"RESULTS_PATH\"]\n",
    "result_files = [p for p in pathlib.Path(RESULTS_PATH).iterdir() if p.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4daf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = []\n",
    "for result_file in result_files:\n",
    "    if f\"_{EXPERIMENT_VERSION}_\" not in result_file.name:\n",
    "        continue\n",
    "    args, all_results = load_results(result_file.name)\n",
    "    base_data = {\n",
    "        \"model\": args[\"model_name\"],\n",
    "        \"variant\": args[\"variant\"],\n",
    "        \"relevance_threshold\": args[\"relevance_threshold\"],\n",
    "        \"confidence_threshold\": args[\"confidence_threshold\"],\n",
    "        \"use_tools\": not args[\"disable_tools\"],\n",
    "    }\n",
    "    if args[\"dataset\"] == \"wiki_contradict_humaneval\":\n",
    "        summary_results.append(\n",
    "            {\n",
    "                **base_data,\n",
    "                \"result\": \"WikiContradict Base\",\n",
    "                \"value\": compute_score_mean(all_results, \"score_satisfies_criteria\"),\n",
    "            }\n",
    "        )\n",
    "    elif args[\"dataset\"] == \"conflict_bank\":\n",
    "        summary_results.append(\n",
    "            {\n",
    "                **base_data,\n",
    "                \"result\": \"ConflictBank Lax\",\n",
    "                \"value\": compute_score_mean(all_results, \"lax_score\"),\n",
    "            }\n",
    "        )\n",
    "        summary_results.append(\n",
    "            {\n",
    "                **base_data,\n",
    "                \"result\": \"ConflictBank Moderate\",\n",
    "                \"value\": compute_score_mean(all_results, \"moderate_score\"),\n",
    "            }\n",
    "        )\n",
    "        summary_results.append(\n",
    "            {\n",
    "                **base_data,\n",
    "                \"result\": \"ConflictBank Strict\",\n",
    "                \"value\": compute_score_mean(all_results, \"strict_score\"),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830dbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(summary_results)\n",
    "df = (\n",
    "    df.pivot(\n",
    "        index=[\n",
    "            \"model\",\n",
    "            \"variant\",\n",
    "            \"relevance_threshold\",\n",
    "            \"confidence_threshold\",\n",
    "            \"use_tools\",\n",
    "        ],\n",
    "        columns=\"result\",\n",
    "        values=\"value\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"model\", \"variant\"], key=lambda col: col.str.lower())\n",
    ")\n",
    "df[\"ConflictBank Mean\"] = df[\n",
    "    [\"ConflictBank Lax\", \"ConflictBank Moderate\", \"ConflictBank Strict\"]\n",
    "].mean(axis=1)\n",
    "df[\"Mean\"] = df[[\"ConflictBank Mean\", \"WikiContradict Base\"]].mean(axis=1)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c600ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>result</th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>relevance_threshold</th>\n",
       "      <th>confidence_threshold</th>\n",
       "      <th>use_tools</th>\n",
       "      <th>ConflictBank Lax</th>\n",
       "      <th>ConflictBank Moderate</th>\n",
       "      <th>ConflictBank Strict</th>\n",
       "      <th>WikiContradict Base</th>\n",
       "      <th>ConflictBank Mean</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>e2e</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>nli</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>e2e</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>nli</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>nli</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>nli</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>e2e</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>nli</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>qa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "result                                   model   variant  relevance_threshold  \\\n",
       "0                                 gpt-oss-120b       e2e                  3.5   \n",
       "1                                 gpt-oss-120b  e2e-base                  3.5   \n",
       "2                                 gpt-oss-120b       nli                  3.5   \n",
       "3                                 gpt-oss-120b        qa                  3.5   \n",
       "4                                  gpt-oss-20b       e2e                  3.5   \n",
       "5                                  gpt-oss-20b  e2e-base                  3.5   \n",
       "6                                  gpt-oss-20b       nli                  3.5   \n",
       "7                                  gpt-oss-20b        qa                  3.5   \n",
       "8                       llama-3.3-70b-instruct       e2e                  3.5   \n",
       "9                       llama-3.3-70b-instruct  e2e-base                  3.5   \n",
       "10                      llama-3.3-70b-instruct       nli                  3.5   \n",
       "11                      llama-3.3-70b-instruct        qa                  3.5   \n",
       "12                      llama-3.3-70b-instruct        qa                  3.5   \n",
       "13      llama-4-maverick-17b-128e-instruct-fp8       e2e                  3.5   \n",
       "14      llama-4-maverick-17b-128e-instruct-fp8  e2e-base                  3.5   \n",
       "15      llama-4-maverick-17b-128e-instruct-fp8       nli                  3.5   \n",
       "16      llama-4-maverick-17b-128e-instruct-fp8        qa                  3.5   \n",
       "17      llama-4-maverick-17b-128e-instruct-fp8        qa                  3.5   \n",
       "18                        Qwen2.5-72B-Instruct       e2e                  3.5   \n",
       "19                        Qwen2.5-72B-Instruct  e2e-base                  3.5   \n",
       "20                        Qwen2.5-72B-Instruct       nli                  3.5   \n",
       "21                        Qwen2.5-72B-Instruct        qa                  3.5   \n",
       "22                        Qwen2.5-72B-Instruct        qa                  3.5   \n",
       "\n",
       "result  confidence_threshold  use_tools  ConflictBank Lax  \\\n",
       "0                        2.0       True              0.70   \n",
       "1                        2.0       True              0.67   \n",
       "2                        2.0      False              0.66   \n",
       "3                        2.0      False              0.95   \n",
       "4                        2.0       True              0.88   \n",
       "5                        2.0       True              0.88   \n",
       "6                        2.0      False              0.66   \n",
       "7                        2.0      False              0.93   \n",
       "8                        2.0       True              0.98   \n",
       "9                        2.0       True              0.99   \n",
       "10                       2.0       True              0.89   \n",
       "11                       2.0      False              0.97   \n",
       "12                       2.0       True              0.98   \n",
       "13                       2.0       True              0.98   \n",
       "14                       2.0       True              0.98   \n",
       "15                       2.0       True              0.81   \n",
       "16                       2.0      False              0.95   \n",
       "17                       2.0       True              0.96   \n",
       "18                       2.0       True              0.97   \n",
       "19                       2.0       True              0.97   \n",
       "20                       2.0       True              0.77   \n",
       "21                       2.0      False              0.97   \n",
       "22                       2.0       True              0.96   \n",
       "\n",
       "result  ConflictBank Moderate  ConflictBank Strict  WikiContradict Base  \\\n",
       "0                        0.63                 0.58                 0.74   \n",
       "1                        0.68                 0.59                 0.76   \n",
       "2                        0.63                 0.55                 0.60   \n",
       "3                        0.69                 0.60                 0.74   \n",
       "4                        0.62                 0.55                 0.78   \n",
       "5                        0.69                 0.57                 0.78   \n",
       "6                        0.68                 0.56                 0.51   \n",
       "7                        0.79                 0.64                 0.74   \n",
       "8                        0.66                 0.61                 0.82   \n",
       "9                        0.85                 0.61                 0.81   \n",
       "10                       0.58                 0.54                 0.55   \n",
       "11                       0.64                 0.56                 0.71   \n",
       "12                       0.66                 0.58                 0.72   \n",
       "13                       0.78                 0.60                 0.81   \n",
       "14                       0.79                 0.60                 0.81   \n",
       "15                       0.55                 0.51                 0.63   \n",
       "16                       0.61                 0.55                 0.75   \n",
       "17                       0.60                 0.54                 0.77   \n",
       "18                       0.65                 0.60                 0.80   \n",
       "19                       0.69                 0.61                 0.81   \n",
       "20                       0.52                 0.51                 0.62   \n",
       "21                       0.74                 0.63                 0.69   \n",
       "22                       0.75                 0.63                 0.71   \n",
       "\n",
       "result  ConflictBank Mean  Mean  \n",
       "0                    0.63  0.69  \n",
       "1                    0.65  0.70  \n",
       "2                    0.62  0.61  \n",
       "3                    0.75  0.75  \n",
       "4                    0.68  0.73  \n",
       "5                    0.71  0.74  \n",
       "6                    0.63  0.57  \n",
       "7                    0.79  0.77  \n",
       "8                    0.75  0.79  \n",
       "9                    0.81  0.81  \n",
       "10                   0.67  0.61  \n",
       "11                   0.72  0.72  \n",
       "12                   0.74  0.73  \n",
       "13                   0.79  0.80  \n",
       "14                   0.79  0.80  \n",
       "15                   0.62  0.62  \n",
       "16                   0.70  0.73  \n",
       "17                   0.70  0.74  \n",
       "18                   0.74  0.77  \n",
       "19                   0.76  0.78  \n",
       "20                   0.60  0.61  \n",
       "21                   0.78  0.73  \n",
       "22                   0.78  0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.precision\", 2):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0b2f2",
   "metadata": {},
   "source": [
    "## ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_eli5_results_for_version(version: str):\n",
    "    load_dotenv()\n",
    "    RESULTS_PATH = os.environ[\"RESULTS_PATH\"]\n",
    "    result_files = [p for p in pathlib.Path(RESULTS_PATH).iterdir() if p.is_file()]\n",
    "\n",
    "    summary_results = []\n",
    "    for result_file in result_files:\n",
    "        if f\"_{version}_\" not in result_file.name:\n",
    "            continue\n",
    "        args, all_results = load_results(result_file.name)\n",
    "        base_data = {\n",
    "            \"evaluated_model\": args[\"evaluated_model_name\"],\n",
    "            \"model\": args[\"model_name\"],\n",
    "            \"variant\": args[\"variant\"],\n",
    "        }\n",
    "        if args[\"dataset\"] == \"eli5_base\":\n",
    "            summary_results.append(\n",
    "                {\n",
    "                    **base_data,\n",
    "                    \"result\": \"ELI5 Base Comprehensiveness\",\n",
    "                    \"value\": compute_score_mean(all_results),\n",
    "                }\n",
    "            )\n",
    "        elif args[\"dataset\"] == \"eli5_v2\":\n",
    "            summary_results.append(\n",
    "                {\n",
    "                    **base_data,\n",
    "                    \"result\": \"ELI5 V2 Comprehensiveness\",\n",
    "                    \"value\": compute_score_mean(all_results),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(summary_results)\n",
    "    df = (\n",
    "        df.pivot(\n",
    "            index=[\n",
    "                \"evaluated_model\",\n",
    "                \"model\",\n",
    "                \"variant\",\n",
    "            ],\n",
    "            columns=\"result\",\n",
    "            values=\"value\",\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\n",
    "            by=[\"evaluated_model\", \"model\", \"variant\"], key=lambda col: col.str.lower()\n",
    "        )\n",
    "    )\n",
    "    df[\"Comprehensiveness Mean\"] = df[\n",
    "        [\"ELI5 Base Comprehensiveness\", \"ELI5 V2 Comprehensiveness\"]\n",
    "    ].mean(axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    with pd.option_context(\"display.precision\", 2):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a266f919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>result</th>\n",
       "      <th>evaluated_model</th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>ELI5 Base Comprehensiveness</th>\n",
       "      <th>ELI5 V2 Comprehensiveness</th>\n",
       "      <th>Comprehensiveness Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>qa</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "result                         evaluated_model        model variant  \\\n",
       "0                                 gpt-oss-120b  gpt-oss-20b      qa   \n",
       "1                                  gpt-oss-20b  gpt-oss-20b      qa   \n",
       "2                       llama-3.3-70b-instruct  gpt-oss-20b      qa   \n",
       "3       llama-4-maverick-17b-128e-instruct-fp8  gpt-oss-20b      qa   \n",
       "4                         Qwen2.5-72B-Instruct  gpt-oss-20b      qa   \n",
       "\n",
       "result  ELI5 Base Comprehensiveness  ELI5 V2 Comprehensiveness  \\\n",
       "0                              0.60                       0.71   \n",
       "1                              0.58                       0.69   \n",
       "2                              0.52                       0.67   \n",
       "3                              0.54                       0.67   \n",
       "4                              0.50                       0.66   \n",
       "\n",
       "result  Comprehensiveness Mean  \n",
       "0                         0.65  \n",
       "1                         0.63  \n",
       "2                         0.59  \n",
       "3                         0.60  \n",
       "4                         0.58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eli5_results_for_version(\"eval_ELI5_gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdd33c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>result</th>\n",
       "      <th>evaluated_model</th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>ELI5 Base Comprehensiveness</th>\n",
       "      <th>ELI5 V2 Comprehensiveness</th>\n",
       "      <th>Comprehensiveness Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>e2e</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "result                         evaluated_model  \\\n",
       "0                                 gpt-oss-120b   \n",
       "1                                  gpt-oss-20b   \n",
       "2                       llama-3.3-70b-instruct   \n",
       "3       llama-4-maverick-17b-128e-instruct-fp8   \n",
       "4                         Qwen2.5-72B-Instruct   \n",
       "\n",
       "result                                   model variant  \\\n",
       "0       llama-4-maverick-17b-128e-instruct-fp8     e2e   \n",
       "1       llama-4-maverick-17b-128e-instruct-fp8     e2e   \n",
       "2       llama-4-maverick-17b-128e-instruct-fp8     e2e   \n",
       "3       llama-4-maverick-17b-128e-instruct-fp8     e2e   \n",
       "4       llama-4-maverick-17b-128e-instruct-fp8     e2e   \n",
       "\n",
       "result  ELI5 Base Comprehensiveness  ELI5 V2 Comprehensiveness  \\\n",
       "0                              0.63                       0.80   \n",
       "1                              0.59                       0.77   \n",
       "2                              0.61                       0.78   \n",
       "3                              0.62                       0.75   \n",
       "4                              0.52                       0.71   \n",
       "\n",
       "result  Comprehensiveness Mean  \n",
       "0                         0.72  \n",
       "1                         0.68  \n",
       "2                         0.69  \n",
       "3                         0.68  \n",
       "4                         0.61  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eli5_results_for_version(\"eval_ELI5_llama-4-maverick-17b-128e-instruct-fp8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf0e241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>result</th>\n",
       "      <th>evaluated_model</th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>ELI5 Base Comprehensiveness</th>\n",
       "      <th>ELI5 V2 Comprehensiveness</th>\n",
       "      <th>Comprehensiveness Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-4-maverick-17b-128e-instruct-fp8</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2.5-72B-Instruct</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>e2e-base</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "result                         evaluated_model                   model  \\\n",
       "0                                 gpt-oss-120b  llama-3.3-70b-instruct   \n",
       "1                                  gpt-oss-20b  llama-3.3-70b-instruct   \n",
       "2                       llama-3.3-70b-instruct  llama-3.3-70b-instruct   \n",
       "3       llama-4-maverick-17b-128e-instruct-fp8  llama-3.3-70b-instruct   \n",
       "4                         Qwen2.5-72B-Instruct  llama-3.3-70b-instruct   \n",
       "\n",
       "result   variant  ELI5 Base Comprehensiveness  ELI5 V2 Comprehensiveness  \\\n",
       "0       e2e-base                         0.47                       0.69   \n",
       "1       e2e-base                         0.45                       0.64   \n",
       "2       e2e-base                         0.50                       0.68   \n",
       "3       e2e-base                         0.51                       0.62   \n",
       "4       e2e-base                         0.45                       0.59   \n",
       "\n",
       "result  Comprehensiveness Mean  \n",
       "0                         0.58  \n",
       "1                         0.54  \n",
       "2                         0.59  \n",
       "3                         0.56  \n",
       "4                         0.52  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eli5_results_for_version(\"eval_ELI5_llama-3-3-70b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb988e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
